# üß† One-vs-Rest ‚Äî Simple Perceptron

Neste reposit√≥rio, realizei a implementa√ß√£o manual daquele que foi o antecessor das redes neurais modernas: o **Perceptron**.

Utilizei conhecimentos te√≥ricos estudados sobre esse algoritmo juntamente com a biblioteca **NumPy**, adotando o paradigma da **Programa√ß√£o Orientada a Objetos**. O objetivo foi criar uma abstra√ß√£o do modelo que pudesse ser ajustada para diferentes necessidades.

---

## ‚ö†Ô∏è Limita√ß√µes do Perceptron

A rede Perceptron possui uma limita√ß√£o importante: ela √© **incapaz de aprender padr√µes de dados n√£o lineares**.

Ainda assim, optei por utilizar o **conjunto de dados Iris** devido √† sua estrutura relativamente pr√≥xima de uma separa√ß√£o linear entre as classes.

---

## üîÑ Classifica√ß√£o Multiclasse com One-vs-Rest

O Perceptron √© naturalmente um classificador **bin√°rio**. Para resolver o problema da **classifica√ß√£o multiclasse** (j√° que o dataset Iris possui tr√™s classes: *setosa*, *versicolor* e *virginica*), utilizei a t√©cnica **One-vs-Rest (OvR)**.

Essa t√©cnica consiste em treinar um modelo para cada classe, distinguindo se a amostra pertence ou n√£o a ela. Por exemplo:

- üü¢ Um Perceptron determina se a amostra √© *setosa* ou n√£o.
- üîµ Um Perceptron determina se a amostra √© *versicolor* ou n√£o.
- üî¥ Um Perceptron determina se a amostra √© *virginica* ou n√£o.

> ‚öôÔ∏è **Na pr√°tica, treinei apenas dois modelos**: um para *setosa* e outro para *virginica*.  
> Se a amostra n√£o for *setosa* nem *virginica*, ela √© *versicolor*.  
> Essa abordagem economiza **tempo e mem√≥ria**.

---

## üìä M√©tricas e Visualiza√ß√£o

Durante o treinamento, a implementa√ß√£o **armazena as m√©tricas em cada √©poca**, possibilitando an√°lises gr√°ficas e comparativas do desempenho do modelo.

M√©tricas salvas:
- ‚úÖ Acur√°cia  
- üìå Precis√£o  
- üîÅ Recall  
- üéØ F1-Score

---

## üß© Funcionamento do Perceptron

![Rede Perceptron](./img/perceptron.png)

Para entender o Perceptron, √© importante compreender primeiro o conceito de **neur√¥nio artificial**. Nosso c√©rebro √© composto por bilh√µes de neur√¥nios conectados. O neur√¥nio artificial √© uma **abstra√ß√£o matem√°tica** desses neur√¥nios biol√≥gicos. Quando interligados, esses neur√¥nios artificiais formam uma **rede neural**.

### üß± Estrutura do Perceptron

1. **Recebe entradas** (valores num√©ricos dos dados)
2. Cada entrada √© **multiplicada por um peso** (import√¢ncia da informa√ß√£o)
3. Soma-se um valor chamado **bias**, que desloca a fun√ß√£o de ativa√ß√£o
4. O resultado passa por uma **fun√ß√£o de ativa√ß√£o**

> üßÆ **Fun√ß√£o degrau**:
> - Se o valor > 0 ‚Üí sa√≠da = 1  
> - Caso contr√°rio ‚Üí sa√≠da = 0

---

### ‚öôÔ∏è Par√¢metros importantes

| Par√¢metro          | Fun√ß√£o                                                                 |
|--------------------|------------------------------------------------------------------------|
| **Pesos**          | Representam a import√¢ncia de cada entrada                              |
| **Bias**           | Valor constante que desloca a fun√ß√£o de ativa√ß√£o                       |
| **Taxa de aprendizado** | Define a velocidade de aprendizado do modelo                        |

- Taxa **alta**: aprendizado r√°pido, risco de **overfitting**
- Taxa **baixa**: aprendizado lento, risco de **underfitting**

---

### üîÅ Processo de Aprendizado

Durante o treinamento:

- Pesos iniciados com **valores pequenos aleat√≥rios**
- Bias geralmente come√ßa como **zero**
- Para cada amostra:

```text
1. resultado = soma(peso * entrada) + bias
2. aplica fun√ß√£o de ativa√ß√£o
3. erro = valor_esperado - valor_previsto
4. novo_peso = peso_antigo + taxa_aprendizado * erro * entrada
5. novo_bias = bias_antigo + taxa_aprendizado * erro
